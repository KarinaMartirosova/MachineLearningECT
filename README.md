# MachineLearningECT
Probability theory and statistical methods play a central role in science. Nowadays we are surrounded by huge amounts of data. For example, there are about one trillion web pages; more than one hour of video is uploaded to YouTube every second, amounting to 10 years of content every day; the genomes of 1000s of people, each of which has a length of more than a billion base pairs, have been sequenced by various labs and so on. This deluge of data calls for automated methods of data analysis, which is exactly what machine learning provides. The purpose of this summer school is to provide an introduction to the core concepts and tools of machine learning in a manner easily understood and intuitive to physicists and nuclear physicists in particular. We will start with some of the basic methods from supervised learning, such as various regression methods before we move into deep learning methods for 
both supervised and unsupervised learning, 
with an emphasis on the analysis of nuclear physics experiments and theoretical nuclear physics. 
Hands-on examples will be provided and the aim is to give the participants 
an overview on how machine learning can be used 
to analyze and study nuclear physics problems.

All learning material and teaching schedule pertinent to the course are avaliable at this GitHub address. A simple _git clone_ of the material gives you access to all lecture notes and program examples. Similarly, running a _git pull_ gives you immediately the latest updates. For an easy visualization of the learning material (html, jupyter-notebooks or plain pdf files), see https://nucleartalent.github.io/MachineLearningECT/doc/web/course.html

## Course content

Probability theory and statistical methods play a central role in science. Nowadays we are
surrounded by huge amounts of data. For example, there are about one trillion web pages; more than one
hour of video is uploaded to YouTube every second, amounting to 10 years of content every
day; the genomes of 1000s of people, each of which has a length of more than a billion  base pairs, have
been sequenced by various labs and so on.
This deluge of data calls for automated methods of data analysis, which is exactly what machine learning provides. 
The purpose of this summer school is to provide an introduction to the core concepts and tools of machine learning in a manner easily understood and intuitive to physicists and nuclear physicists in particular. We will start with some of the basic methods from supervised learning, such as various regression methods before we move into deep learning methods for both supervised and unsupervised learning, with an emphasis on the analysis of nuclear physics experiments and theoretical nuclear physics. 
Hands-on examples will be provided and the aim is to give the participants an overview on how machine learning can be used to analyze and study nuclear physics problems. 

###  The following topics will be covered
- Basic concepts, expectation values, variance, covariance, correlation functions and errors;
- Estimation of errors using cross-validation, blocking, bootstrapping and jackknife methods;
- Optimization of functions
- Linear Regression and Logistic Regression;
- Boltzmann machines;
- Neural networks and deep learning;
- Convolutional Neural Networks
- Recurrent Neureal Networks and Autoencoders
- Decisions trees and random forests
- Support vector machines and kernel transformations
- Dimensionality reduction techniques, from supervised to unsupervised learning

All the above topics will be supported by examples, hands-on exercises and project work.


## Practicalities

1. Three lectures per day, starting at 9am, see schedule below
2. Hands-on sessions in the afternoons till 6pm



## Possible textbooks

_Recommended textbook_:
- Aurelien Geron, Hands‑On Machine Learning with Scikit‑Learn and TensorFlow, O'Reilly

_General learning book on statistical analysis_:
- Christian Robert and George Casella, Monte Carlo Statistical Methods, Springer
- Peter Hoff, A first course in Bayesian statistical models, Springer
- Trevor Hastie, Robert Tibshirani, Jerome H. Friedman, The Elements of Statistical Learning, Springer
_General Machine Learning Books_:
- Kevin Murphy, Machine Learning: A Probabilistic Perspective, MIT Press
- Christopher M. Bishop, Pattern Recognition and Machine Learning, Springer
- David J.C. MacKay, Information Theory, Inference, and Learning Algorithms, Cambridge University Press
- David Barber, Bayesian Reasoning and Machine Learning, Cambridge University Press 

## Schedule

Lectures are approximately 45 min and there is a small break of 15 min between each lecture. Longer breaks at 1030am-11am and 3pm-330pm.
Acronyms for teachers
- DZ = Daniel Bazin
- MHJ = Morten Hjorth-Jensen
- MK = Michelle Kuchera
- SL = Sean Liddick
- RR = Raghuram Ramanujan
